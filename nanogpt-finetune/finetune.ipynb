{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d1406d1",
   "metadata": {},
   "source": [
    "## Fine-tuning GPT-2 With your data. \n",
    "\n",
    "This notebook demonstrates how to fine-tune the smallest GPT-2 model (124M parameters) on your custom text data. Unlike training from scratch, fine-tuning starts with a pre-trained model and adapts it to your needs.\n",
    "\n",
    "Important: Make sure you run every cell in this workbook by using the \"Play\" button on the right-hand side of each cell before moving on to the next one.\n",
    "If you have to restart the program for some reason, you might have to run the cells again.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7438512",
   "metadata": {},
   "source": [
    "## Prepare Your Data\n",
    "Place your text data in a file called input.txt in the same directory as this notebook. The text should be clean and representative of what you want the model to learn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e76e876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 148043 characters from alice.txt\n",
      "Train: 38141 tokens, Val: 4189 tokens, Vocab: 50257 tokens\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "from contextlib import nullcontext\n",
    "from model import GPT, GPTConfig\n",
    "\n",
    "# Load and prepare your data\n",
    "input_file = 'alice.txt'\n",
    "if not os.path.exists(input_file):\n",
    "    raise FileNotFoundError(f\"Please ensure {input_file} exists in the current directory\")\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f\"Loaded {len(text)} characters from {input_file}\")\n",
    "\n",
    "# Create configuration (using Shakespeare example parameters)\n",
    "dataset = input_file.split('.')[0]\n",
    "out_dir = f'out-{dataset}-finetune'\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "# Process and tokenize data\n",
    "n = len(text)\n",
    "train_data = text[:int(n*0.9)]\n",
    "val_data = text[int(n*0.9):]\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "train_ids = enc.encode_ordinary(train_data)\n",
    "val_ids = enc.encode_ordinary(val_data)\n",
    "vocab_size = enc.n_vocab\n",
    "print(f\"Train: {len(train_ids)} tokens, Val: {len(val_ids)} tokens, Vocab: {vocab_size} tokens\")\n",
    "\n",
    "# Save data as binary files\n",
    "train_file = f'{dataset}_train.bin'\n",
    "val_file = f'{dataset}_val.bin'\n",
    "train_ids = np.array(train_ids, dtype=np.uint16)\n",
    "val_ids = np.array(val_ids, dtype=np.uint16)\n",
    "train_ids.tofile(train_file)\n",
    "val_ids.tofile(val_file)\n",
    "\n",
    "# Training settings\n",
    "batch_size = 1\n",
    "gradient_accumulation_steps = 32\n",
    "block_size = 1024\n",
    "learning_rate = 3e-5\n",
    "max_iters = 20\n",
    "eval_interval = 5\n",
    "eval_iters = 40\n",
    "decay_lr = False\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "dtype = 'float16' if torch.cuda.is_available() else 'float32'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d29693",
   "metadata": {},
   "source": [
    "## Define utility functions and load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b33abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model from 'gpt2'...\n",
      "loading weights from pretrained gpt: gpt2\n",
      "forcing vocab_size=50257, block_size=1024, bias=True\n",
      "overriding dropout rate to 0.0\n",
      "number of parameters: 123.65M\n",
      "Model has 124.44M parameters\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "GPT.configure_optimizers() missing 1 required positional argument: 'device_type'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(p.numel()\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mp\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mmodel.parameters())/\u001b[32m1e6\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mM parameters\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Optimizer setup\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m optimizer = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfigure_optimizers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbetas\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.95\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Context manager for mixed precision\u001b[39;00m\n\u001b[32m     38\u001b[39m ptdtype = {\u001b[33m'\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m'\u001b[39m: torch.float32, \u001b[33m'\u001b[39m\u001b[33mbfloat16\u001b[39m\u001b[33m'\u001b[39m: torch.bfloat16, \u001b[33m'\u001b[39m\u001b[33mfloat16\u001b[39m\u001b[33m'\u001b[39m: torch.float16}[dtype]\n",
      "\u001b[31mTypeError\u001b[39m: GPT.configure_optimizers() missing 1 required positional argument: 'device_type'"
     ]
    }
   ],
   "source": [
    "# Define utility functions\n",
    "def get_batch(split):\n",
    "    data = np.memmap(train_file if split == 'train' else val_file, dtype=np.uint16, mode='r')\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# Load and configure the model\n",
    "print(f\"Initializing model from 'gpt2'...\")\n",
    "model = GPT.from_pretrained('gpt2', dict(dropout=0.0))\n",
    "model = model.to(device)\n",
    "print(f\"Model has {sum(p.numel() for p in model.parameters())/1e6:.2f}M parameters\")\n",
    "\n",
    "# Optimizer setup\n",
    "optimizer = model.configure_optimizers(\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=0.1,\n",
    "    betas=(0.9, 0.95),\n",
    "    device=device,\n",
    "    dtype=dtype,\n",
    "    decay_lr=decay_lr\n",
    ")\n",
    "\n",
    "# Context manager for mixed precision\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = torch.amp.autocast(device_type=device, dtype=ptdtype) if device != 'cpu' else nullcontext()\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16' and device == 'cuda'))\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "best_val_loss = float('inf')\n",
    "print(f\"Starting training for {max_iters} iterations...\")\n",
    "start_time = time.time()\n",
    "\n",
    "for iter_num in range(max_iters):\n",
    "    # Forward and backward passes with gradient accumulation\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    # Print start of iteration\n",
    "    print(f\"Iteration {iter_num+1}/{max_iters} - starting... \", end='', flush=True)\n",
    "    iter_start_time = time.time()\n",
    "    \n",
    "    for micro_step in range(gradient_accumulation_steps):\n",
    "        X, Y = get_batch('train')\n",
    "        with ctx:\n",
    "            logits, loss = model(X, Y)\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "    \n",
    "    # Gradient clipping and optimizer step\n",
    "    scaler.unscale_(optimizer)\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    \n",
    "    # Calculate time for this iteration\n",
    "    iter_time = time.time() - iter_start_time\n",
    "    print(f\"completed in {iter_time:.2f}s (loss: {loss.item()*gradient_accumulation_steps:.4f})\")\n",
    "    \n",
    "    # Evaluation and checkpoint saving (keep the existing eval_interval logic)\n",
    "    if (iter_num + 1) % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"EVAL - iter {iter_num+1}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        \n",
    "        if losses['val'] < best_val_loss:\n",
    "            best_val_loss = losses['val']\n",
    "            checkpoint = {\n",
    "                'model': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'model_args': model.config,\n",
    "                'iter_num': iter_num,\n",
    "                'best_val_loss': best_val_loss\n",
    "            }\n",
    "            print(f\"Saving checkpoint to {out_dir}\")\n",
    "            torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\nTraining completed in {elapsed:.2f} seconds!\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb87e9ad",
   "metadata": {},
   "source": [
    "## Generate some text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16715226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: 'Alice was'\n",
      "--------------------------------------------------\n",
      "Alice was smiling in a very happy way, and looking at the lake, she continued:\n",
      "'Every piece of bread has a bit of dust in it, and so is every dish of bread.'\n",
      "'That's all,' said the Cook, who had stopped to listen.\n",
      "'Yes, the little bivalves,' said the waitress, who was standing beside the table.\n",
      "'Why?' said the cook, who had been sitting down on one of the cots, and who was looking very politely at the table.\n",
      "'It's nothing,' said the waitress, who sat down again, and looked very kindly at the little bivalves.\n",
      "'What do you think the bivalves are?'\n",
      "'They're very curious.'\n",
      "'Really curious?' said the waitress, who had been holding the table-clot.\n",
      "'Yes,' said the cook; 'and the bivalves are very curious. They tell you the story of a golden-haired bivalve\n",
      "==================================================\n",
      "\n",
      "Prompt: 'The Queen'\n",
      "--------------------------------------------------\n",
      "The Queen's hat was belted down, and her arm was pricked with a crown of the same color. \"I'm sorry I ever had a crown of such color, but I can't remember it,\" said the tailor.\n",
      "\n",
      "\"Gentlemen, I did remember it,\" said the Queen, \"with a little growing weeping, but it's not quite enough to be heard again.\"\n",
      "\n",
      "\"But what matters is that it doesn't taste good,\" said the Queen.\n",
      "\n",
      "\"I have no idea what that means,\" said the tailor.\n",
      "\n",
      "\"I shouldn't say it, your Majesty,\" said the Queen; \"but it's nothing very old then, as you see. Do you think I'll remember it?\"\n",
      "\n",
      "\"No, I won't,\" said the tailor; \"but, on the contrary, I shall remember it as my first hat, and that is nothing but a smooth, well-dressed, elegant grandchild of the Queen's.\"\n",
      "\n",
      "==================================================\n",
      "\n",
      "Prompt: 'Down the rabbit'\n",
      "--------------------------------------------------\n",
      "Down the rabbit down the rabbit's throat and out of the air, the size of a 5-fish little sister took her place at the top of the door.\n",
      "\n",
      "\"Come in, I'll let you in!\" cried her mother.\n",
      "\n",
      "\"Take away your bacon,\" cried the others, and they all turned to look at the little girl.\n",
      "\n",
      "\"What the f---!\" cried the voice.\n",
      "\n",
      "\"I won't eat you, I'm going to eat you,\" answered the little girl.\n",
      "\n",
      "Pleasure was the only thing left, and she turned round so entirely that she could not see the garden garden.\n",
      "\n",
      "She walked about, back and forth, until she came to the garden, and her dear little sister turned to her and said,\n",
      "\"W-What's wrong? Take off your bacon!\"\n",
      "\n",
      "\"It won't help you, baby,\" replied the other.\n",
      "\n",
      "\"I'd rather I didn't eat you, I never did,\"\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Generate some text with the trained model\n",
    "def generate_text(prompt, max_tokens=200):\n",
    "    model.eval()\n",
    "    tokens = enc.encode_ordinary(prompt)\n",
    "    x = torch.tensor([tokens], dtype=torch.long, device=device)\n",
    "    with torch.no_grad():\n",
    "        y = model.generate(x, max_new_tokens=max_tokens, temperature=0.8)\n",
    "    return enc.decode(y[0].tolist())\n",
    "\n",
    "# Test with a few prompts\n",
    "test_prompts = [\"Alice was\", \"The Queen\", \"Down the rabbit\"]\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\nPrompt: '{prompt}'\")\n",
    "    print(\"-\" * 50)\n",
    "    generated = generate_text(prompt)\n",
    "    print(generated)\n",
    "    print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
