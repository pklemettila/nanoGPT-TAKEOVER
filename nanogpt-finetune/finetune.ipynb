{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d1406d1",
   "metadata": {},
   "source": [
    "## Fine-tuning GPT-2 With your data. \n",
    "\n",
    "This notebook demonstrates how to fine-tune the smallest GPT-2 model (124M parameters) on your custom text data. Unlike training from scratch, fine-tuning starts with a pre-trained model and adapts it to your needs.\n",
    "\n",
    "Important: Make sure you run every cell in this workbook by using the \"Play\" button on the right-hand side of each cell before moving on to the next one.\n",
    "If you have to restart the program for some reason, you might have to run the cells again.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7438512",
   "metadata": {},
   "source": [
    "## Prepare Your Data\n",
    "Place your text data in a file called input.txt in the same directory as this notebook. The text should be clean and representative of what you want the model to learn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4e76e876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 148043 characters from alice.txt\n",
      "Train: 38141 tokens, Val: 4189 tokens, Vocab: 50257 tokens\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "from contextlib import nullcontext\n",
    "from model import GPT, GPTConfig\n",
    "\n",
    "# Load and prepare your data\n",
    "input_file = 'alice.txt'\n",
    "if not os.path.exists(input_file):\n",
    "    raise FileNotFoundError(f\"Please ensure {input_file} exists in the current directory\")\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f\"Loaded {len(text)} characters from {input_file}\")\n",
    "\n",
    "# Create configuration (using Shakespeare example parameters)\n",
    "dataset = input_file.split('.')[0]\n",
    "out_dir = f'out-{dataset}-finetune'\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "# Process and tokenize data\n",
    "n = len(text)\n",
    "train_data = text[:int(n*0.9)]\n",
    "val_data = text[int(n*0.9):]\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "train_ids = enc.encode_ordinary(train_data)\n",
    "val_ids = enc.encode_ordinary(val_data)\n",
    "vocab_size = enc.n_vocab\n",
    "print(f\"Train: {len(train_ids)} tokens, Val: {len(val_ids)} tokens, Vocab: {vocab_size} tokens\")\n",
    "\n",
    "# Save data as binary files\n",
    "train_file = f'{dataset}_train.bin'\n",
    "val_file = f'{dataset}_val.bin'\n",
    "train_ids = np.array(train_ids, dtype=np.uint16)\n",
    "val_ids = np.array(val_ids, dtype=np.uint16)\n",
    "train_ids.tofile(train_file)\n",
    "val_ids.tofile(val_file)\n",
    "\n",
    "# Training settings\n",
    "batch_size = 1\n",
    "gradient_accumulation_steps = 32\n",
    "block_size = 1024\n",
    "learning_rate = 3e-5\n",
    "max_iters = 20\n",
    "eval_interval = 5\n",
    "eval_iters = 40\n",
    "decay_lr = False\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "dtype = 'float16' if torch.cuda.is_available() else 'float32'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d29693",
   "metadata": {},
   "source": [
    "## Define utility functions and load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a3b33abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model from 'gpt2'...\n",
      "loading weights from pretrained gpt: gpt2\n",
      "forcing vocab_size=50257, block_size=1024, bias=True\n",
      "overriding dropout rate to 0.0\n",
      "number of parameters: 123.65M\n",
      "Model has 124.44M parameters\n",
      "num decayed parameter tensors: 50, with 124,318,464 parameters\n",
      "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
      "using fused AdamW: True\n",
      "Starting training for 20 iterations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cydas\\AppData\\Local\\Temp\\ipykernel_21792\\4226022714.py:39: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16' and device == 'cuda'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 5: train loss 2.8029, val loss 2.8485\n",
      "Saving checkpoint to out-alice-finetune\n",
      "iter 10: train loss 2.6709, val loss 2.8459\n",
      "Saving checkpoint to out-alice-finetune\n",
      "iter 15: train loss 2.5099, val loss 2.7683\n",
      "Saving checkpoint to out-alice-finetune\n",
      "iter 20: train loss 2.4738, val loss 2.7775\n",
      "\n",
      "Training completed in 37.10 seconds!\n",
      "Best validation loss: 2.7683\n"
     ]
    }
   ],
   "source": [
    "# Define utility functions\n",
    "def get_batch(split):\n",
    "    data = np.memmap(train_file if split == 'train' else val_file, dtype=np.uint16, mode='r')\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# Load and configure the model\n",
    "print(f\"Initializing model from 'gpt2'...\")\n",
    "model = GPT.from_pretrained('gpt2', dict(dropout=0.0))\n",
    "model = model.to(device)\n",
    "print(f\"Model has {sum(p.numel() for p in model.parameters())/1e6:.2f}M parameters\")\n",
    "\n",
    "# Optimizer setup\n",
    "optimizer = model.configure_optimizers(\n",
    "    weight_decay=1e-2, learning_rate=learning_rate, \n",
    "    betas=(0.9, 0.95), device_type=device\n",
    ")\n",
    "\n",
    "# Context manager for mixed precision\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = torch.amp.autocast(device_type=device, dtype=ptdtype) if device != 'cpu' else nullcontext()\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16' and device == 'cuda'))\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "best_val_loss = float('inf')\n",
    "print(f\"Starting training for {max_iters} iterations...\")\n",
    "start_time = time.time()\n",
    "\n",
    "for iter_num in range(max_iters):\n",
    "    # Learning rate scheduling\n",
    "    if decay_lr:\n",
    "        lr = learning_rate * (0.5 ** (iter_num / max_iters))\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    \n",
    "    # Forward and backward passes with gradient accumulation\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    for micro_step in range(gradient_accumulation_steps):\n",
    "        X, Y = get_batch('train')\n",
    "        with ctx:\n",
    "            logits, loss = model(X, Y)\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "    \n",
    "    # Gradient clipping and optimizer step\n",
    "    scaler.unscale_(optimizer)\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    \n",
    "    # Evaluation and checkpoint saving\n",
    "    if (iter_num + 1) % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"iter {iter_num+1}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        \n",
    "        if losses['val'] < best_val_loss:\n",
    "            best_val_loss = losses['val']\n",
    "            checkpoint = {\n",
    "                'model': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'model_args': model.config,\n",
    "                'iter_num': iter_num,\n",
    "                'best_val_loss': best_val_loss\n",
    "            }\n",
    "            print(f\"Saving checkpoint to {out_dir}\")\n",
    "            torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\nTraining completed in {elapsed:.2f} seconds!\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb87e9ad",
   "metadata": {},
   "source": [
    "## Generate some text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "16715226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: 'Alice was'\n",
      "--------------------------------------------------\n",
      "Alice was in the doorway when she heard the door open. She thought she heard someone banging on the window, but nothing happened.\n",
      "\"I can't tell you what happened,\" said the doorkeeper, as she opened it. \"What did you do? Did you go hunting?\"\n",
      "\"I wasn't hunting.\"\n",
      "\"Yes, you were. Was that all you wanted?\"\n",
      "She thought she heard the door close behind her, and she looked, as if she had opened it.\n",
      "\"It was only a little after midnight, there's nothing you can tell me about it.\"\n",
      "\"What did I do wrong?\"\n",
      "She looked down at the ten-dollar bill and thought she saw a guard with a question mark above it, but she could see nothing but guards.\n",
      "\"I haven't seen him since nightfall, and I was wondering what was going on. I came upon a little schoolhouse in the middle of the street, and the doors only opened when you was in the\n",
      "==================================================\n",
      "\n",
      "Prompt: 'The Queen'\n",
      "--------------------------------------------------\n",
      "The Queen must, for the greater part, keep silent,' and so the Jubilee parade was abandoned, and the King and Queen went round the country, to see if no one had got at their feet or their hopes. But the Queen was not quite sure.\n",
      "'I know it,' she said.\n",
      "'I thought you might see Mrs. Mercer, but she'd been gone for so long!' said the Jubilee; and he went off again, and sat down.\n",
      "The Duchess of Buckingham, who was sitting in the garden, was carrying a little rake, and a little rose, and very nearly a bat; and she said to herself,â€”'Is there any more children to bring into the garden?'\n",
      "'Well, the Duchess of Buckingham, I'm afraid nobody's the least bit sure,' said the King; and he looked at her.\n",
      "'I'd like to see you at one of your favourite places,' said the Queen, with a smile.\n",
      "'I think I\n",
      "==================================================\n",
      "\n",
      "Prompt: 'Down the rabbit'\n",
      "--------------------------------------------------\n",
      "Down the rabbit, and you can see the golden, black-legged creature.\n",
      "Breathe into it.\n",
      "That's how you use your nose - breath into it.\n",
      "That's it.\n",
      "I don't know what kind of a nose you use, but I think it is nice. What am I to do?\n",
      "Come, let's talk.\n",
      "How about a little more talk?\n",
      "I'll give you some more of your own, and I'll ask you a question.\n",
      "How about you?\n",
      "Forgive me if I am going to ask, but I'm not going to ask at all.\n",
      "But look at this -- look at that face, it's a little ugly.\n",
      "That's what all the best of you do when you're so young.\n",
      "You are the first and last of your kind, and I'm the last.\n",
      "I am nothing like that.\n",
      "You thought I was old enough to be your kind, but nobody ever thought it could be\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Generate some text with the trained model\n",
    "def generate_text(prompt, max_tokens=200):\n",
    "    model.eval()\n",
    "    tokens = enc.encode_ordinary(prompt)\n",
    "    x = torch.tensor([tokens], dtype=torch.long, device=device)\n",
    "    with torch.no_grad():\n",
    "        y = model.generate(x, max_new_tokens=max_tokens, temperature=0.8)\n",
    "    return enc.decode(y[0].tolist())\n",
    "\n",
    "# Test with a few prompts\n",
    "test_prompts = [\"Alice was\", \"The Queen\", \"Down the rabbit\"]\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\nPrompt: '{prompt}'\")\n",
    "    print(\"-\" * 50)\n",
    "    generated = generate_text(prompt)\n",
    "    print(generated)\n",
    "    print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
