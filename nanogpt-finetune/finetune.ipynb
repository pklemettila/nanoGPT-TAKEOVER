{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d1406d1",
   "metadata": {},
   "source": [
    "## Fine-tuning GPT-2 With your data. \n",
    "\n",
    "This notebook demonstrates how to fine-tune the smallest GPT-2 model (124M parameters) on your custom text data. Unlike training from scratch, fine-tuning starts with a pre-trained model and adapts it to your needs.\n",
    "\n",
    "Important: Make sure you run every cell in this workbook by using the \"Play\" button on the right-hand side of each cell before moving on to the next one.\n",
    "If you have to restart the program for some reason, you might have to run the cells again.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7438512",
   "metadata": {},
   "source": [
    "## Prepare Your Data\n",
    "Place your text data in a file called input.txt in the same directory as this notebook. The text should be clean and representative of what you want the model to learn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e76e876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset in characters: 148,043\n",
      "First 500 characters of your data:\n",
      "ï»¿Alice's Adventures in Wonderland\n",
      "\n",
      "                ALICE'S ADVENTURES IN WONDERLAND\n",
      "\n",
      "                          Lewis Carroll\n",
      "\n",
      "               THE MILLENNIUM FULCRUM EDITION 3.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                            CHAPTER I\n",
      "\n",
      "                      Down the Rabbit-Hole\n",
      "\n",
      "\n",
      "  Alice was beginning to get very tired of sitting by her sister\n",
      "on the bank, and of having nothing to do:  once or twice she had\n",
      "peeped into the book her sister was reading, but it had no\n",
      "pictures or conversations in it, `and what is t\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import importlib.util\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the project root to Python path if needed\n",
    "# sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "# Check input data\n",
    "input_file = 'alice.txt'\n",
    "if not os.path.exists(input_file):\n",
    "    raise FileNotFoundError(f\"Please ensure {input_file} exists in the current directory\")\n",
    "\n",
    "print(f\"Fine-tuning with data from: {input_file}\")\n",
    "\n",
    "# Create configuration\n",
    "config = {\n",
    "    # I/O\n",
    "    'out_dir': f'out-{input_file.split(\".\")[0]}-finetune',\n",
    "    'eval_interval': 5,\n",
    "    'eval_iters': 40,\n",
    "    'always_save_checkpoint': False,\n",
    "    'init_from': 'gpt2',\n",
    "    \n",
    "    # wandb (optional)\n",
    "    'wandb_log': False,\n",
    "    'wandb_project': f'{input_file.split(\".\")[0]}-finetune',\n",
    "    'wandb_run_name': f'ft-{time.time()}',\n",
    "    \n",
    "    # data\n",
    "    'dataset': input_file.split('.')[0],\n",
    "    'gradient_accumulation_steps': 32,\n",
    "    'batch_size': 1,\n",
    "    'block_size': 1024,\n",
    "    \n",
    "    # optimizer\n",
    "    'learning_rate': 3e-5,\n",
    "    'max_iters': 20,\n",
    "    'weight_decay': 1e-2,\n",
    "    'beta1': 0.9,\n",
    "    'beta2': 0.95,\n",
    "    'grad_clip': 1.0,\n",
    "    \n",
    "    # learning rate decay\n",
    "    'decay_lr': False,\n",
    "    'warmup_iters': 0,\n",
    "    'lr_decay_iters': 20,  # same as max_iters\n",
    "    'min_lr': 3e-6,  # 1/10th of learning_rate\n",
    "    \n",
    "    # system\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'dtype': 'float16',\n",
    "    'compile': False,\n",
    "}\n",
    "\n",
    "print(f\"Configuration ready for fine-tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d29693",
   "metadata": {},
   "source": [
    "## Configure your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3b33abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "  Output directory: out-alice-finetune-notebook\n",
      "  Dataset: alice\n",
      "  Device: cuda, dtype: float16, compile: False\n",
      "  Max iterations: 20\n",
      "  Learning rate: 3e-05\n"
     ]
    }
   ],
   "source": [
    "out_dir = 'out-alice-finetune-notebook'\n",
    "eval_interval = 5\n",
    "log_interval = 1\n",
    "eval_iters = 40 # \n",
    "always_save_checkpoint = True # Save checkpoint if validation loss improves\n",
    "init_from = 'gpt2' # 'scratch' or 'resume' or 'gpt2*'\n",
    "max_iters = 20\n",
    "\n",
    "# wandb logging (optional)\n",
    "wandb_log = False\n",
    "wandb_project = 'alice-finetune-notebook'\n",
    "wandb_run_name = 'ft-notebook-' + str(time.time()) # Requires import time if not already done\n",
    "\n",
    "# Data\n",
    "dataset = input_file.split('.')[0] # e.g., 'alice' from 'alice.txt'\n",
    "gradient_accumulation_steps = 32\n",
    "batch_size = 1 # Phsyical batch size\n",
    "block_size = 1024\n",
    "\n",
    "# Model (these are for 'gpt2' and will be overridden by from_pretrained, but good for reference)\n",
    "n_layer = 12\n",
    "n_head = 12\n",
    "n_embd = 768\n",
    "dropout = 0.0 # For fine-tuning, can increase if overfitting, 0.0 for gpt2 pretrain\n",
    "bias = False # GPT-2 uses bias, but nanoGPT model.py allows False\n",
    "\n",
    "# AdamW Optimizer\n",
    "learning_rate = 3e-5  # Main learning rate       # Total number of training iterations.\n",
    "weight_decay = 1e-2\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "grad_clip = 1.0 # Clip gradients at this value, or 0.0 to disable\n",
    "\n",
    "# Learning rate decay settings\n",
    "decay_lr = True # Whether to decay the learning rate\n",
    "warmup_iters = 0 # How many steps to warm up for, 0 for fine-tuning is often ok\n",
    "lr_decay_iters = max_iters # Should be ~= max_iters per Chinchilla\n",
    "min_lr = learning_rate / 10 # Minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n",
    "\n",
    "# System\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16'\n",
    "dtype = 'float16' # For broader compatibility, use float16. Change to bfloat16 if your GPU supports it and you prefer.\n",
    "# Note: float16 requires a GradScaler. bfloat16 typically does not.\n",
    "compile_model = False # Requires PyTorch 2.0+\n",
    "\n",
    "# Ensure output directory exists\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "print(f\"Configuration loaded:\")\n",
    "print(f\"  Output directory: {out_dir}\")\n",
    "print(f\"  Dataset: {dataset}\")\n",
    "print(f\"  Device: {device}, dtype: {dtype}, compile: {compile_model}\")\n",
    "print(f\"  Max iterations: {max_iters}\")\n",
    "print(f\"  Learning rate: {learning_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1030f219",
   "metadata": {},
   "source": [
    "## Tokenize the Data\n",
    "\n",
    "Instead of direct integer tokenization, we use *TikTok* which is also used by OpenAI to tokenize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "60b0cc93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train has 38,141 tokens\n",
      "Validation has 4,189 tokens\n",
      "Vocabulary size: 50,257 tokens\n",
      "Created alice_train.bin and alice_val.bin binary files in the current directory.\n"
     ]
    }
   ],
   "source": [
    "n = len(text)\n",
    "train_data = text[:int(n*0.9)]\n",
    "val_data = text[int(n*0.9):]\n",
    "\n",
    "# Encode with tiktoken GPT-2 BPE (exactly like nanoGPT)\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "train_ids = enc.encode_ordinary(train_data)\n",
    "val_ids = enc.encode_ordinary(val_data)\n",
    "\n",
    "print(f\"Train has {len(train_ids):,} tokens\")\n",
    "print(f\"Validation has {len(val_ids):,} tokens\")\n",
    "\n",
    "# Get vocabulary size from the tokenizer\n",
    "vocab_size = enc.n_vocab\n",
    "print(f\"Vocabulary size: {vocab_size:,} tokens\")\n",
    "\n",
    "# Export to binary files (named after your dataset)\n",
    "dataset_name = input_file.split('.')[0]  # e.g., 'alice' from 'alice.txt'\n",
    "global_train_file = f'{dataset}_train.bin'\n",
    "global_val_file = f'{dataset}_val.bin'\n",
    "\n",
    "train_ids = np.array(train_ids, dtype=np.uint16)\n",
    "val_ids = np.array(val_ids, dtype=np.uint16)\n",
    "train_ids.tofile(global_train_file)\n",
    "val_ids.tofile(global_val_file)\n",
    "\n",
    "print(f\"Created {global_train_file} and {global_val_file} binary files in the current directory.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f41b52",
   "metadata": {},
   "source": [
    "## Step 3: Set up batching\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4f08e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input tokens: [10010, 13679, 319, 543, 262, 41857, 367, 1436]\n",
      "Sample target tokens: [13679, 319, 543, 262, 41857, 367, 1436, 198]\n",
      "Decoded input:  concert!' on which the wretched Hatter\n"
     ]
    }
   ],
   "source": [
    "def get_batch(split, batch_size_gb=1, block_size_gb=1024): # Renamed params to avoid conflict if config vars are global\n",
    "    \"\"\"Load a batch from the binary files (nanoGPT style)\"\"\"\n",
    "    # Use dataset-specific file names (now using global_train_file, global_val_file)\n",
    "    if split == 'train':\n",
    "        data = np.memmap(global_train_file, dtype=np.uint16, mode='r')\n",
    "    else:\n",
    "        data = np.memmap(global_val_file, dtype=np.uint16, mode='r')\n",
    "    \n",
    "    # Generate random starting positions\n",
    "    ix = torch.randint(len(data) - block_size_gb, (batch_size_gb,))\n",
    "    \n",
    "    # Create input and target sequences\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size_gb]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size_gb]).astype(np.int64)) for i in ix])\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "x_test, y_test = get_batch('train', batch_size_gb=batch_size, block_size_gb=8) # Use config batch_size\n",
    "\n",
    "print(\"Sample input tokens:\", x_test[0].tolist())\n",
    "print(\"Sample target tokens:\", y_test[0].tolist())\n",
    "print(\"Decoded input:\", enc.decode(x_test[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4869a180",
   "metadata": {},
   "source": [
    "## Step 4: Configuring device & Loading the model\n",
    "\n",
    "smol GPT - precursor to what we know as ChatGPT in a sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f58fe673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading pre-trained model: gpt2...\n",
      "loading weights from pretrained gpt: gpt2\n",
      "forcing vocab_size=50257, block_size=1024, bias=True\n",
      "overriding dropout rate to 0.0\n",
      "number of parameters: 123.65M\n",
      "Model has 124.44M parameters\n",
      "Model configuration:\n",
      "  - Layers: 12\n",
      "  - Heads: 12\n",
      "  - Embedding dimension: 768\n",
      "  - Block size: 1024\n",
      "  - Vocabulary size: 50257\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu' # Now from config\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize model from pre-trained GPT-2 (using nanoGPT's architecture)\n",
    "print(f\"Loading pre-trained model: {init_from}...\")\n",
    "if init_from == 'scratch':\n",
    "    # init a new model from scratch\n",
    "    print(\"Initializing a new model from scratch\")\n",
    "    gptconf = GPTConfig(block_size=block_size, vocab_size=vocab_size, n_layer=n_layer, n_head=n_head, n_embd=n_embd, dropout=dropout, bias=bias)\n",
    "    model = GPT(gptconf)\n",
    "elif init_from.startswith('gpt2'):\n",
    "    model = GPT.from_pretrained(init_from, dict(dropout=dropout))\n",
    "else:\n",
    "    print(f\"Resuming training from {init_from}\")\n",
    "    # TODO: Implement resume logic if needed, similar to train.py\n",
    "    ckpt_path = os.path.join(out_dir, init_from if init_from != 'resume' else 'ckpt.pt')\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "    gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "    model = GPT(gptconf)\n",
    "    state_dict = checkpoint['model']\n",
    "    # fix state dict keys if needed (e.g. DDP)\n",
    "    unwanted_prefix = '_orig_mod.'\n",
    "    for k,v in list(state_dict.items()):\n",
    "        if k.startswith(unwanted_prefix):\n",
    "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "# model.train() # Will be set in training loop\n",
    "\n",
    "if compile_model:\n",
    "    print(\"Compiling the model... (takes a ~minute)\")\n",
    "    try:\n",
    "        model = torch.compile(model) # requires PyTorch 2.0\n",
    "        print(\"Model compiled successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Model compilation failed: {e}. Proceeding without compilation.\")\n",
    "        compile_model = False # Fallback\n",
    "\n",
    "print(f\"Model has {sum(p.numel() for p in model.parameters())/1e6:.2f}M parameters\")\n",
    "\n",
    "print(f\"Model configuration:\")\n",
    "print(f\"  - Layers: {model.config.n_layer}\")\n",
    "print(f\"  - Heads: {model.config.n_head}\")  \n",
    "print(f\"  - Embedding dimension: {model.config.n_embd}\")\n",
    "print(f\"  - Block size: {model.config.block_size}\")\n",
    "print(f\"  - Vocabulary size: {model.config.vocab_size}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed2a3b9",
   "metadata": {},
   "source": [
    "## Step 5: Set hyperparameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a53e05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective batch size: 32\n",
      "Context length (block_size): 1024 tokens\n",
      "Initial learning rate: 3e-05\n",
      "num decayed parameter tensors: 50, with 124,318,464 parameters\n",
      "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
      "using fused AdamW: True\n",
      "Testing evaluation function...\n",
      "Initial training loss: 3.0043\n",
      "Initial validation loss: 3.1948\n"
     ]
    }
   ],
   "source": [
    "# Training hyperparameters (following nanoGPT's fine-tuning approach)\n",
    "batch_size = 1                      # Small physical batch size for memory efficiency\n",
    "gradient_accumulation_steps = 32    # Accumulate gradients (effective batch = 32)\n",
    "block_size = 1024                   # Full GPT-2 context length\n",
    "learning_rate = 3e-5                # Conservative fine-tuning rate\n",
    "eval_interval = 5                   # Evaluate every 5 iterations\n",
    "eval_iters = 40                     # Number of iterations for evaluation\n",
    "\n",
    "print(f\"Effective batch size: {batch_size * gradient_accumulation_steps}\")\n",
    "print(f\"Context length (block_size): {block_size} tokens\")\n",
    "print(f\"Initial learning rate: {learning_rate}\")\n",
    "\n",
    "# Create optimizer (following nanoGPT's approach)\n",
    "optimizer = model.configure_optimizers(\n",
    "    weight_decay=weight_decay, # from config\n",
    "    learning_rate=learning_rate, # from config\n",
    "    betas=(beta1, beta2), # from config\n",
    "    device_type=device # from config\n",
    ")\n",
    "\n",
    "# @torch.no_grad() ... estimate_loss function remains the same for now\n",
    "# ... rest of the cell (estimate_loss and its test call) ...\n",
    "# Make sure get_batch in estimate_loss uses the correct block_size and batch_size from config\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    \"\"\"Evaluate the model on both training and validation data\"\"\"\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters) # uses config eval_iters\n",
    "        for k in range(eval_iters):\n",
    "            # Pass config batch_size and block_size\n",
    "            X, Y = get_batch(split, batch_size_gb=batch_size, block_size_gb=block_size)\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "            # Autocast for mixed precision if not CPU\n",
    "            ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "            ctx = torch.amp.autocast(device_type=device, dtype=ptdtype) if device != 'cpu' else nullcontext()\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# Test evaluation function\n",
    "print(\"Testing evaluation function...\")\n",
    "test_losses = estimate_loss()\n",
    "print(f\"Initial training loss: {test_losses['train']:.4f}\")\n",
    "print(f\"Initial validation loss: {test_losses['val']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a92c47",
   "metadata": {},
   "source": [
    "## Step 6: Learning rate scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a516c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters: # from config\n",
    "        return learning_rate * it / warmup_iters # learning_rate, warmup_iters from config\n",
    "    # 2) if it > lr_decay_iters, return min_lr\n",
    "    if it > lr_decay_iters: # from config\n",
    "        return min_lr # from config\n",
    "    # 3) in between, use cosine decay down to min_lr\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
    "    return min_lr + coeff * (learning_rate - min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc95616c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fine-tuning for 20 iterations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cydas\\AppData\\Local\\Temp\\ipykernel_21792\\2101480837.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16' and device == 'cuda'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 5: train loss 2.7919, val loss 2.9311, lr 2.60e-05, time 6.06s\n",
      "saving checkpoint to out-alice-finetune-notebook\n",
      "step 10: train loss 2.6741, val loss 2.8495, lr 1.65e-05, time 13.67s\n",
      "saving checkpoint to out-alice-finetune-notebook\n",
      "step 15: train loss 2.6604, val loss 2.8628, lr 6.95e-06, time 21.09s\n",
      "saving checkpoint to out-alice-finetune-notebook\n",
      "\n",
      "Fine-tuning completed!\n",
      "Total time: 27.6 seconds (20 iterations performed in this run)\n",
      "Final training loss: 2.5456\n",
      "Final validation loss: 2.7631\n",
      "Best validation loss achieved: 2.8628\n",
      "Processing speed: 0.73 iterations per second\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 110\u001b[39m\n\u001b[32m    107\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProcessing speed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocal_iter_num\u001b[38;5;250m \u001b[39m/\u001b[38;5;250m \u001b[39melapsed\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m iterations per second\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    109\u001b[39m \u001b[38;5;66;03m# Plotting losses (optional)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m iter_log: \u001b[38;5;66;03m# Check if any evaluations were done\u001b[39;00m\n\u001b[32m    112\u001b[39m     plt.figure(figsize=(\u001b[32m10\u001b[39m, \u001b[32m4\u001b[39m))\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import time # ensure time is imported\n",
    "\n",
    "print(f\"Starting fine-tuning for {max_iters} iterations...\")\n",
    "\n",
    "# Autocast and GradScaler setup\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = torch.amp.autocast(device_type=device, dtype=ptdtype) if device != 'cpu' else nullcontext()\n",
    "# GradScaler for float16\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16' and device == 'cuda'))\n",
    "\n",
    "start_time = time.time()\n",
    "iter_num = 0 # Use iter_num to align with train.py\n",
    "best_val_loss = float('inf')\n",
    "local_iter_num = 0 # Number of iterations for this training run\n",
    "\n",
    "# Track metrics for analysis (can be expanded)\n",
    "train_losses_log = []\n",
    "val_losses_log = []\n",
    "iter_log = []\n",
    "\n",
    "model.train() # Ensure model is in training mode\n",
    "\n",
    "# Training loop\n",
    "for iter_num in range(max_iters): # Loop up to max_iters\n",
    "    \n",
    "    # Determine and set the learning rate for this iteration\n",
    "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    # Evaluate the model on train/val sets and write checkpoints\n",
    "    if iter_num % eval_interval == 0 and iter_num > 0 : # Use config eval_interval\n",
    "        losses = estimate_loss()\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, lr {lr:.2e}, time {elapsed_time:.2f}s\")\n",
    "        \n",
    "        train_losses_log.append(losses['train'].item()) # .item() to get float\n",
    "        val_losses_log.append(losses['val'].item())\n",
    "        iter_log.append(iter_num)\n",
    "\n",
    "        if wandb_log:\n",
    "            try:\n",
    "                wandb.log({\n",
    "                    \"iter\": iter_num,\n",
    "                    \"train/loss\": losses['train'],\n",
    "                    \"val/loss\": losses['val'],\n",
    "                    \"lr\": lr,\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Wandb logging failed: {e}\")\n",
    "        \n",
    "        if losses['val'] < best_val_loss or always_save_checkpoint: # always_save_checkpoint from config\n",
    "            best_val_loss = losses['val']\n",
    "            if iter_num > 0:\n",
    "                checkpoint = {\n",
    "                    'model': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'model_args': model.config, # Save GPTConfig object directly\n",
    "                    'iter_num': iter_num,\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                    'config': {k: v for k, v in globals().items() if k in ['learning_rate', 'batch_size', 'block_size', 'dataset', 'max_iters', 'dropout', 'grad_clip', 'weight_decay', 'beta1', 'beta2', 'n_layer', 'n_head', 'n_embd']}, # save relevant config\n",
    "                }\n",
    "                print(f\"saving checkpoint to {out_dir}\")\n",
    "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
    "                if losses['val'] == best_val_loss and not always_save_checkpoint: # only print if it's a new best and not always saving\n",
    "                     print(f\"  â Saved new best model (val_loss: {best_val_loss:.4f})\")\n",
    "\n",
    "\n",
    "    # Forward backward update, with gradient accumulation...\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    for micro_step in range(gradient_accumulation_steps): # gradient_accumulation_steps from config\n",
    "        X, Y = get_batch('train', batch_size_gb=batch_size, block_size_gb=block_size) # Use config vars\n",
    "        X, Y = X.to(device), Y.to(device)\n",
    "        with ctx:\n",
    "            logits, loss = model(X, Y)\n",
    "            loss = loss / gradient_accumulation_steps # Scale loss for accumulation\n",
    "        \n",
    "        # Backward pass, with gradient scaling if training in fp16\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "    # Clip gradients\n",
    "    if grad_clip != 0.0: # grad_clip from config\n",
    "        scaler.unscale_(optimizer) # unscale before clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    \n",
    "    # Step the optimizer and scaler if training in fp16\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    \n",
    "    local_iter_num +=1\n",
    "\n",
    "    if iter_num >= max_iters:\n",
    "        break\n",
    "\n",
    "# Final evaluation\n",
    "elapsed = time.time() - start_time\n",
    "final_losses = estimate_loss()\n",
    "\n",
    "print(f\"\\nFine-tuning completed!\")\n",
    "print(f\"Total time: {elapsed:.1f} seconds ({local_iter_num} iterations performed in this run)\")\n",
    "print(f\"Final training loss: {final_losses['train']:.4f}\")\n",
    "print(f\"Final validation loss: {final_losses['val']:.4f}\")\n",
    "print(f\"Best validation loss achieved: {best_val_loss:.4f}\")\n",
    "print(f\"Processing speed: {local_iter_num / elapsed:.2f} iterations per second\")\n",
    "\n",
    "# Plotting losses (optional)\n",
    "import matplotlib.pyplot as plt\n",
    "if iter_log: # Check if any evaluations were done\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(iter_log, train_losses_log, label='Train Loss')\n",
    "    plt.plot(iter_log, val_losses_log, label='Validation Loss')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Loss Over Iterations')\n",
    "    plt.savefig(os.path.join(out_dir, 'loss_plot.png'))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a21bb8e",
   "metadata": {},
   "source": [
    "## Step 7: Generating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ae5ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing text generation with different prompts:\n",
      "\n",
      "Prompt: 'Alice was'\n",
      "--------------------------------------------------\n",
      "Alice was to cut off his nose, and he did not look at her as she had done an hour ago, and he said to himself: \"Augh! how may I be so angry as this little dog? How can I possibly be so angry at a dog?\"\n",
      "\n",
      "\"Why, now that I think about it, you're a little bit mad!! I must see you again! You may be ill-tempered, very much so! Or you might as well be gentle and kind to a dog, as very mad: and you'll have to begin from the beginning.\"\n",
      "\n",
      "\"I must say I have rather great surprise in me than any things I've ever used to do: and I've said it to a great many people that I feel very little respect for dogs, that I think only some of them would ever be very, very fond of Indian dogs, and that, if any of them were to do it, I would be very glad to have them in\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Prompt: 'The Queen'\n",
      "--------------------------------------------------\n",
      "The Queen of Heartsâ¦\"\n",
      "\n",
      "\"Yes, Queen of Heartsâ¦\"\n",
      "\n",
      "Once a person has gone on, it can't be helped from remembering that the Queen of Hearts was the Queen of Heartsâ¦\n",
      "\n",
      "She was the Queen of Hearts.\n",
      "\n",
      "Right, you know that sort of thing.\n",
      "\n",
      "You are the Queen of Hearts.\n",
      "\n",
      "You can be anything you want.\n",
      "\n",
      "You can be anything you want.\n",
      "\n",
      "You can be anything you want.\n",
      "\n",
      "You can be anything you wantâ¦\n",
      "\n",
      "There was a time when it was called the \"Queen of Hearts\"\n",
      "and she was the Queen of Hearts.\n",
      "And now, the Queen of Hearts is gone.\n",
      "There's no place to return.\n",
      "She's gone.\n",
      "There's no place for us to be.\n",
      "But we want to be somewhere else.\n",
      "So we can do things that are just as good for us as they are for you.\n",
      "Only, we only do things that are\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Prompt: 'Down the rabbit'\n",
      "--------------------------------------------------\n",
      "Down the rabbit's back, and the serpent rushed into the room. The room was extremely crowded with people as the men and the dogs ran round it.\n",
      "A young Tabby got into the same room and began to tell the story.\n",
      "\"You're the only one here, my dear, of the noblest integrity. An old master's first choice, you know. An old master's first choice.\"\n",
      "\"An old master's first choice, I repeat, I repeat!\"\n",
      "\"It's the first choice of the three! In the first place, it's made of brass! There's no box!\"\n",
      "\"No box! You shall not do that!\"\n",
      "\"And where is the box? Where is the missing key? Why, no key there!\"\n",
      "\"You shall not do that!\"\n",
      "\"And where is the key? Where is the missing key? Why, no key there!\"\n",
      "\"The key, of course, begins with the key-pond!\"\n",
      "\"\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_text(prompt_text, model_to_use, max_new_tokens=500, temperature=1.0, top_k=None):\n",
    "    # ... (rest of the function is the same, but ensure it uses model_to_use) ...\n",
    "    # Encode the prompt text into tokens\n",
    "    tokens = enc.encode_ordinary(prompt_text)\n",
    "    \n",
    "    # Convert to tensor and move to the correct device\n",
    "    context = torch.tensor([tokens], dtype=torch.long, device=device)\n",
    "    \n",
    "    # Generate new tokens using nanoGPT's generate method\n",
    "    # model.eval() # model_to_use should already be in eval mode\n",
    "    with torch.no_grad():\n",
    "        generated = model_to_use.generate(context, max_new_tokens=max_new_tokens, \n",
    "                                 temperature=temperature, top_k=top_k)\n",
    "    # ... (rest of the function) ...\n",
    "    generated_tokens = generated[0].tolist()\n",
    "    generated_text = enc.decode(generated_tokens)\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "# Test generation with different prompts based on your dataset\n",
    "if 'alice' in dataset_name.lower():\n",
    "    test_prompts = [\"Alice was\", \"The Queen\", \"Down the rabbit\"]\n",
    "elif 'shakespeare' in dataset_name.lower():\n",
    "    test_prompts = [\"To be or not\", \"Romeo and\", \"All the world\"]\n",
    "else:\n",
    "    test_prompts = [\"Once upon a time\", \"In the beginning\", \"The story\"]\n",
    "\n",
    "print(\"Testing text generation with different prompts:\\n\")\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"Prompt: '{prompt}'\")\n",
    "    print(\"-\" * 50)\n",
    "    generated = generate_text(prompt, max_new_tokens=200, temperature=0.8)\n",
    "    print(generated)\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
