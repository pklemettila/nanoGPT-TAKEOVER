{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa67f760",
   "metadata": {},
   "source": [
    "## Preparing a dataset for NanoGPT\n",
    "\n",
    "Important: Make sure you run every cell in this workbook by using the \"Play\" button on the left-hand side of each node before moving on to the next one. \n",
    "\n",
    "If you have to restart the program for some reason, you might have to run the cells again.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132b6d22",
   "metadata": {},
   "source": [
    "For any Machine Learning work, we need some data. Let's use the full text of the classic book \"Alice in Wonderland\" by Lewis Carroll."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9630a86a",
   "metadata": {},
   "source": [
    "Now, let's print some of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa37db19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  148043\n",
      "                            CHAPTER I\n",
      "\n",
      "                      Down the Rabbit-Hole\n",
      "\n",
      "\n",
      "  Alice was beginning to get very tired of sitting by her sister\n",
      "on the bank, and of having nothing to do:  once or twice she had\n",
      "peeped into the book her sister was reading, but it had no\n",
      "pictures or conversations in it, `and what is the use of a book,'\n",
      "thought Alice `without pictures or conversation?'\n",
      "\n",
      "  So she was considering in her own mind (as well as she could,\n",
      "for the hot day made her feel very sleepy and stupid), whether\n",
      "the pleasure of making a daisy-chain would be worth the trouble\n",
      "of getting up and picking the daisies, when suddenly a White\n",
      "Rabbit with pink eyes ran close by her.\n",
      "\n",
      "  There was nothing so VERY remarkable in that; nor did Alice\n",
      "think it so VERY much out of the way to hear the Rabbit say to\n",
      "itself, `Oh \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Change \"placeholder.txt\" below \n",
    "with open('alice.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Let's see how many characters are in the text\n",
    "print(\"length of dataset in characters: \", len(text))\n",
    "\n",
    "# Let's print some of the text\n",
    "print(text[180:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31697ab7",
   "metadata": {},
   "source": [
    "Since our language model will operate on character level, let's start by listing all the unique characters that appear in the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88bf69d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: False\n",
      "all the unique characters in the text:  \n",
      " !\"'()*,-.03:;?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]`abcdefghijklmnopqrstuvwxyzï»¿\n",
      "72 different characters\n"
     ]
    }
   ],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "\n",
    "print(\"all the unique characters in the text: \", ''.join(chars))\n",
    "print(str(vocab_size) + \" different characters\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df177c2",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Now we'll need a way to *tokenize* the text. Simply put, computers work more effectively with numbers than with letters or characters. This process is called *tokenization*, and is an essential part of how language models operate. There are different ways to do this, and tokenization gets more complex with more advanced language models like ChatGPT. \n",
    "\n",
    "However, as we are building a character-level language model, we can adopt a simple approach: we are going to convert each character into an unique integer (...-3,-2,-1,0,1,2,3...). \n",
    "\n",
    "Of course, we won't assign these numbers manually, that would be way too much trouble: Python makes it easy for us to automate this process using a few functions.\n",
    "\n",
    "*If you don't know anything about programming, don't panic: that isn't the focus here - just run the code using the \"Play\" button, observe the results and move on.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21925e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = { ch:i for i,ch in enumerate(chars) } # character to integer mapping (stoi is short for \"string to integer\")\n",
    "itos = { i:ch for i,ch in enumerate(chars) } # integer to character mapping (itos is short for \"integer to string\")\n",
    "\n",
    "# Function to convert a string into a list of integers (encoding)\n",
    "def encode(text):\n",
    "    result = []\n",
    "    for character in text:\n",
    "        result.append(stoi[character])\n",
    "    return result\n",
    "\n",
    "# Function to convert a list of integers back into a string (decoding)\n",
    "def decode(integers):\n",
    "    result = ''\n",
    "    for index in integers:\n",
    "        result += itos[index]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84ed050",
   "metadata": {},
   "source": [
    "Let's test out the encoding function. Feel feel to change the text inside encode(\"\") to be whatever you like and try it out. Note that since our program only knows the characters that were used in the input text we downloaded earlier, using some non-letter characters may crash the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1a5aec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded letter 'a': [45]\n",
      "encoded letter 'b': [46]\n",
      "encoded letter 'A': [16]\n",
      "encoded letter 'B': [17]\n",
      "Hello World = [23, 49, 56, 56, 59, 1, 38, 59, 62, 56, 48, 2]\n",
      "What is the capital of Assyria? [59, 56, 48, 63, 47, 52, 59, 59, 56, 1, 49, 57, 59, 64, 49, 63, 1, 45, 62, 49, 1, 47, 59, 59, 56, 49, 62, 1, 64, 52, 45, 58, 1, 49, 57, 59, 54, 53, 63, 1, 68, 19]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "encoded_letter_1 = encode(\"a\")\n",
    "encoded_letter_2 = encode(\"b\")\n",
    "\n",
    "# Note that uppercase letters are treated as different characters\n",
    "encoded_letter_3 = encode(\"A\")\n",
    "encoded_letter_4 = encode(\"B\")\n",
    "\n",
    "encoded_text_1 = encode(\"Hello World!\")\n",
    "\n",
    "encoded_text_2 = encode(\"oldschool emotes are cooler than emojis xD\")\n",
    "print(\"encoded letter 'a': \" + str(encoded_letter_1))\n",
    "print(\"encoded letter 'b': \" + str(encoded_letter_2))  \n",
    "print(\"encoded letter 'A': \" + str(encoded_letter_3))\n",
    "print(\"encoded letter 'B': \" + str(encoded_letter_4))\n",
    "\n",
    "print(\"Hello World = \" + str(encoded_text_1))\n",
    "print(\"What is the capital of Assyria? \" + str(encoded_text_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf850f0",
   "metadata": {},
   "source": [
    "Let's also test out the decoding function. You can try changing some of the numbers inside `decode([])` to see how it converts back to text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5db3abf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "Hello World!\n",
      "oldschool emotes are cooler than emojis xD\n"
     ]
    }
   ],
   "source": [
    "decoded_letter_1 = decode(encoded_letter_1) \n",
    "\n",
    "decoded_text_1 = decode(encoded_text_1)\n",
    "\n",
    "decoded_text_2 = decode(encoded_text_2)\n",
    "\n",
    "print(decoded_letter_1)\n",
    "print(decoded_text_1)\n",
    "print(decoded_text_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "095ac704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!\n",
      "oldschool emotes are cooler than emojis xD\n"
     ]
    }
   ],
   "source": [
    "decoded_text_1 = decode(encoded_text_1)\n",
    "print(decoded_text_1)\n",
    "\n",
    "decoded_text_2 = decode(encoded_text_2)\n",
    "print(decoded_text_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7ddcd9",
   "metadata": {},
   "source": [
    "### Encoding the full dataset\n",
    "Now, let's apply that same **encoding and decoding** logic to the entire dataset we downloaded earlier. We're storing the data in a **Torch**, which is a multi-dimensional array that can be computed efficiently on graphics cards - this is basically what enables modern AI to be as effective as it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e6b1f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing first 200 characters of the encoded text dataset\n",
      "torch.Size([148043]) torch.int64\n",
      "tensor([71, 16, 56, 53, 47, 49,  4, 63,  1, 16, 48, 66, 49, 58, 64, 65, 62, 49,\n",
      "        63,  1, 53, 58,  1, 38, 59, 58, 48, 49, 62, 56, 45, 58, 48,  0,  0,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, 16, 27, 24,\n",
      "        18, 20,  4, 34,  1, 16, 19, 37, 20, 29, 35, 36, 33, 20, 34,  1, 24, 29,\n",
      "         1, 38, 30, 29, 19, 20, 33, 27, 16, 29, 19,  0,  0,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1, 27, 49, 67, 53, 63,  1, 18, 45, 62, 62, 59, 56, 56,  0,  0,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, 35, 23, 20,\n",
      "         1, 28, 24, 27, 27, 20, 29, 29, 24, 36, 28,  1, 21, 36, 27, 18, 33, 36,\n",
      "        28,  1, 20, 19, 24, 35, 24, 30, 29,  1, 12, 10, 11,  0,  0,  0,  0,  0,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1])\n",
      "You have now successfully encoded the entiredataset into a tensor of integers.\n"
     ]
    }
   ],
   "source": [
    "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
    "import torch # we use PyTorch: https://pytorch.org\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "print(\"printing first 200 characters of the encoded text dataset\")\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:200]) # the 1000 characters we looked at earlier will to the GPT look like this\n",
    "print(\"You have now successfully encoded the entiredataset into a tensor of integers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22095e20",
   "metadata": {},
   "source": [
    "Soon it is time to train our \"baby\" GPT model. However, we don't want to train with all of our data - we don't want it to create an exact copy of the input text, we want it to create text that is similar to it in style. So, we will withhold 10% of the dataset for *validation data* that we will compare the output with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2173749a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now split up the data into train and validation sets\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421fada0",
   "metadata": {},
   "source": [
    "## Saving the files \n",
    "Now that we have processed our text data and split it into training and validation sets, we need to save this data for future use in our GPT model training. \n",
    "\n",
    "We're going to save our data in a special format called \"binary\" - think of it like compressing a file to make it smaller and faster to open. This is important because:\n",
    "\n",
    "- **Smaller files**: Binary format takes up much less space on your computer\n",
    "- **Faster loading**: When we train our AI, it can read these files much quicker\n",
    "- **Better for math**: Earlier, we converted letters to numbers. However, we want to compress it further because computers are naturally better at working with numbers that are in binary format, as that is the format in which all computing occurs at the base level!\n",
    "\n",
    "We'll also save a \"dictionary\" file (meta.pkl) that remembers which number represents which character. Without this, our AI wouldn't know how to translate its number predictions back into readable text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7273507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([71, 16, 56, 53, 47, 49,  4, 63,  1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's now split up the data into train and validation sets\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45a9097",
   "metadata": {},
   "source": [
    "Here we define our Biggram language model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad37d7f",
   "metadata": {},
   "source": [
    "## Defining our model\n",
    "We are using a text prediction model that we import from model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff0c8b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 72 unique characters\n",
      "Training will run on: cpu\n",
      "Model has 0.21M parameters\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from model import BabyGPT  # Import the model from separate file\n",
    "\n",
    "# hyperparameters\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "batch_size = 32    \n",
    "block_size = 64     \n",
    "eval_interval = 5\n",
    "eval_iters = 100\n",
    "dropout = 0.1 # this means that 10% of the neurons will be randomly set to zero during training       \n",
    "n_embd = 64          \n",
    "n_head = 4           \n",
    "n_layer = 4          \n",
    "learning_rate = 1.5e-3\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size} unique characters\")\n",
    "print(f\"Training will run on: {device}\")\n",
    "\n",
    "# Initialize model with the vocabulary size\n",
    "torch.manual_seed(1337)\n",
    "model = BabyGPT(vocab_size, block_size, n_embd, n_head, n_layer, dropout)\n",
    "m = model.to(device)\n",
    "print(f\"Model has {sum(p.numel() for p in m.parameters())/1e6:.2f}M parameters\")\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da85f71c",
   "metadata": {},
   "source": [
    "## Run this to begin training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a9dba89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 5.0s, Iteration: 64, Training prediction error: 2.5811, Generalization error: 2.6050\n",
      "Time: 10.0s, Iteration: 102, Training prediction error: 2.4724, Generalization error: 2.5009\n",
      "Time: 15.0s, Iteration: 141, Training prediction error: 2.4190, Generalization error: 2.4394\n",
      "Time: 20.0s, Iteration: 173, Training prediction error: 2.3600, Generalization error: 2.3747\n",
      "Time: 25.1s, Iteration: 206, Training prediction error: 2.3303, Generalization error: 2.3518\n",
      "Time: 30.0s, Iteration: 238, Training prediction error: 2.2862, Generalization error: 2.3086\n",
      "Time: 35.1s, Iteration: 272, Training prediction error: 2.2110, Generalization error: 2.2305\n",
      "Time: 40.0s, Iteration: 303, Training prediction error: 2.1516, Generalization error: 2.1604\n",
      "Time: 45.0s, Iteration: 335, Training prediction error: 2.0828, Generalization error: 2.1084\n",
      "Time: 50.0s, Iteration: 368, Training prediction error: 2.0153, Generalization error: 2.0506\n",
      "Time: 55.0s, Iteration: 399, Training prediction error: 1.9618, Generalization error: 2.0007\n",
      "Time: 60.0s, Iteration: 430, Training prediction error: 1.9156, Generalization error: 1.9558\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Set training time in seconds (e.g., 5 minutes = 300 seconds)\n",
    "max_train_time = 300  \n",
    "\n",
    "# Create PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Track metrics for visualization later\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "timestamps = []\n",
    "\n",
    "# Get starting time\n",
    "start_time = time.time()\n",
    "iter_count = 0\n",
    "\n",
    "# Train until time limit is reached\n",
    "while (time.time() - start_time) < max_train_time:\n",
    "    # Check if it's time to evaluate\n",
    "    current_time = time.time() - start_time\n",
    "    if int(current_time) % eval_interval == 0 and int(current_time) > 0:\n",
    "        \n",
    "        # Store current time elapsed in seconds\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        # Evaluate model\n",
    "        losses = estimate_loss()\n",
    "        print(f\"Time: {elapsed:.1f}s, Iteration: {iter_count}, Training prediction error: {losses['train']:.4f}, Generalization error: {losses['val']:.4f}\")\n",
    "        \n",
    "        # Store metrics for potential visualization\n",
    "        timestamps.append(elapsed)\n",
    "        train_losses.append(losses['train'])\n",
    "        val_losses.append(losses['val'])\n",
    "\n",
    "    \n",
    "    # Regular training iteration\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    iter_count += 1\n",
    "\n",
    "# Final evaluation\n",
    "elapsed = time.time() - start_time\n",
    "losses = estimate_loss()\n",
    "print(f\"Final results after {elapsed:.1f} seconds ({iter_count} iterations):\")\n",
    "print(f\"Training data prediction error: {losses['train']:.4f}, New text prediction error: {losses['val']:.4f}\")\n",
    "\n",
    "print(f\"Your system processed {iter_count / elapsed:.2f} iterations per second\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bb8a30",
   "metadata": {},
   "source": [
    "Train loss/prediction error = how well the model predicts the next character in the text it has seen before\n",
    "Validation error = how well the model predicts the next character in the text it hasn't seen before\n",
    "\n",
    "Rough Conversion to Accuracy\n",
    "Lower values = better predictions\n",
    "A loss of 1.0 â 60-70% accuracy in next character prediction\n",
    "A loss of 2.0 â 40-50% accuracy\n",
    "A loss of 3.0 â 20-30% accuracy\n",
    "Random guessing would give a loss around 4.2-4.5 (with ~1-2% accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a7e203",
   "metadata": {},
   "source": [
    "## Function to generate text based on a prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f302b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt_text, max_new_tokens=500):\n",
    "    \"\"\"\n",
    "    Generate text from a plain text prompt\n",
    "    \n",
    "    Parameters:\n",
    "    - prompt_text: The starting text for generation\n",
    "    - max_new_tokens: How many characters to generate\n",
    "    \n",
    "    Returns:\n",
    "    - The generated text including the original prompt\n",
    "    \"\"\"\n",
    "    # Check if all characters in the prompt are in our vocabulary\n",
    "    for char in prompt_text:\n",
    "        if char not in stoi:\n",
    "            print(f\"Warning: Character '{char}' not in vocabulary. Replacing with a space.\")\n",
    "            prompt_text = prompt_text.replace(char, \" \")\n",
    "    \n",
    "    # Encode the prompt text into tokens\n",
    "    tokens = encode(prompt_text)\n",
    "    \n",
    "    # Convert to tensor and move to the correct device\n",
    "    context = torch.tensor([tokens], dtype=torch.long, device=device)\n",
    "    \n",
    "    # Generate new tokens\n",
    "    generated_tokens = m.generate(context, max_new_tokens=max_new_tokens)[0].tolist()\n",
    "    \n",
    "    # Decode back to text\n",
    "    generated_text = decode(generated_tokens)\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "# Example usage - replace \"Alice was\" with any starting text\n",
    "print(generate_text(\"Alice was\", max_new_tokens=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46156084",
   "metadata": {},
   "source": [
    "### References\n",
    "Adapted from Andrej Karpathy's (https://www.youtube.com/watch?v=kCc8FmEb1nY) video and accompanying notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
